\documentclass[../main.tex]{subfiles}

\begin{document}

We consider the problem of how to encourage a Reinforcement Learning agent to explore the environment in a uniform manner. To accomplish this, we borrow the method of Local Outlier Factors from Unsupervised Learning and adapt it to serve as a reward signal for an agent to incentivizes the agent to explore parts of the environment it has not heavily explored yet. We show that incentivizing the agent in this way causes it to explore more uniformly than random exploration in one modified gridworld domain. We also provide some evidence to show that using the $k$-distance instead of the LOF results in less uniform exploration in this domain. Altogether this result serves as a strong motivation for further work in this area.

There is a significant quantity of additional work required to make this method into a practical, usable method and to understand it better. Firstly, only one domain has been considered so far. While this is sufficient to show a proof of concept it does not give any firm intuition for what kinds of domains this is a useful method. Secondly, as Local Outlier Factors diverge to infinity when identical data points are allowed, work needs to be conducted to determine if values can be safely clipped or whether something more sophisticated is required to deal with this problem. Thirdly, a more efficient data structure that allows efficient insertion of data points and efficient $k$-NN queries needs to be found and implemented to understand precisely how expensive this method is. Fourthly, as mentioned briefly in Section~\ref{sec:methods}, the idea of using a limited number of data points need further investigation. There may even be some form of geometric decay for the importance of data points that could provide a robust metric that is similar to the LOF metric but more able to deal with a changing environment. Lastly, similarly to the first consideration above, more work needs to be done to understand what domains a uniform exploration strategy works well. Perhaps more importantly, more work needs to be done to determine how this compares to the exploration policy induced by other state of the art methods such as pseudo-count based exploration \cite{DBLP:conf/nips/BellemareSOSSM16}.

\end{document}
