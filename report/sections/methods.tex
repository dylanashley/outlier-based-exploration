\documentclass[../main.tex]{subfiles}

\begin{document}

If we want a Reinforcement Learning to explore the environment uniformly, we ideally want to reward the agent for seeing parts of the environment it has not seen frequently. Local Outlier Factors provide a scalar value for each data point that indicates to what degree each point is an outlier. However, an outlier is a rare data point. So it seems natural that we need only reward a Reinforcement Learning agent with the value provided by Local Outlier Factors to encourage good exploration. This idea is the intuition behind why we could use outliers as a way of guiding exploration.  There are, however, a few questions that must be answered to determine how this can be done. Firstly, we must determine what represents parts of the environment so that we can determine what we can use as data points for the LOF method. Secondly, we must determine what constitutes similarity when referring to two parts of the environment so that there is a dissimilarity metric for the LOF method. Lastly, we must determine how we can adapt the values produced by the LOF method so that it can serve as a suitable reward for an Reinforcement Learning agent. We provide answers to these three questions in the following paragraphs.

The first problem is the easiest: to represent parts of the environment we need only use the feature vectors as data points. However, this raises the question of whether uniform exploration means that all states are visited roughly equally or whether it means that all transitions between two states are experienced roughly equally. Both are valid interpretations, and for the latter case we can simply append the feature vector for the next state to the feature vector for the current state and use this as coordinates for locations in the environment. One advantage of these as ways of representing parts of the environment is that both of these representations have an equally straightforward dissimilarity metric.

In Reinforcement Learning similar states should, in general, have similar feature vectors. Hence the term feature in feature vector. So using the simple Euclidean distance between the vectors is a good choice for a dissimilarity metric of two states. While this is not the only metric that could be used, it is the most straightforward one. It also works equally well if we opt to use transitions rather than states. While this resolves one problem, there is still the problem of how the LOF value must be adapted, so it is suitable as a reward for an Reinforcement Learning agent.

The reward signal in Reinforcement Learning is interpreted the same way we consider rewards in our lives. In other words, a positive reward indicates a good action, a negative reward indicates a bad action, and a reward of zero indicates a neutral action.  Furthermore the value it outputs for each data point is a positive value where values around $1$ indicate a data point is not an outlier and values much greater than one indicate a data point is an outlier.  So to make LOF values suitable for an Reinforcement Learning agent, we begin by subtracting one so that no reward would be given to the agent if it visits an area of the environment it has seen frequently. To reduce the noise in the resulting signal, we opt instead to use the exponential of the absolute value of the new signal so that the reward given to the agent is roughly equal when exploring well-explored areas of the environment and the reward given to the agent is substantial when exploring badly explored areas. While using the absolute value treats different LOF values differently it is unimportant as the exponential effectively suppresses values affected by this. While using the exponential has benefits, it again rewarded the agent for visiting an area of the environment it has frequently seen so we must again subtract one from the result to produce our reward signal. We write the resulting reward signal with states as data points as follows:
\begin{equation}
    \label{eqn:state_reward}
    R_t = e^{|LOF(S_t) - 1|} - 1
\end{equation}

If we instead use transitions as data points we write the resulting reward signal as follows:
\begin{equation}
    \label{eqn:transition_reward}
    R_t = e^{|LOF([S_{t - 1}, S_t]) - 1|} - 1
\end{equation}

The most significant advantage of this method is that it merely produces a reward signal that helps to guide exploration. Since it merely produces a reward signal, it can be used with any algorithm in the Reinforcement Learning literature and can be easily combined with other methods of guiding exploration. For example we could consider a three-tiered system where the agent selects a greedy action with respect to the real expected reward with $1 - \epsilon$ probability, a greedy action with respect to the LOF based reward signal with $\epsilon(1 - \epsilon)$ probability, and a random action with $\epsilon^2$ probability. However, as it modifies the reward signal, to learn an optimal policy with under the true reward signal it is necessary to use two learners. Here one learner learns the values of the states or state action pairs under the LOF based reward signal and one the other learner learns the values of the states or state action pairs under the true reward signal. Using two learners in this way implies that the learner that learns using the true reward signal must use an offpolicy algorithm. This is a requirement of most exploration strategies and such algorithms are abundant. Furthermore while this effectively double the amount of computation required, calculating the LOFs of states or transitions is significantly more expensive than all but the most expensive such algorithms. There remains a way to alleviate this computation overhead somewhat though.

While we do not explore it further, it is not essential that the LOF based reward we use to be calculated with respect to all previous states visited or transitions experienced. Indeed we could choose to calculate it using only the newest $n$ data points. Using only the $n$ newest data points would have two key advantages. Firstly, it would allow us to deal with environments that change as time progresses. Secondly, it would bound the amount of computation required for calculating the reward signal to give to the agent. Both of these remain important considerations in Reinforcement Learning. However, such an improvement comes at a price. In this case, a key disadvantage of this extension is that it introduces a new hyperparameter that must be tuned.

\end{document}
